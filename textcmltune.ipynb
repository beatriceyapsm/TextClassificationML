{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to C:\\Users\\Yee\n",
      "[nltk_data]     ling\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Yee\n",
      "[nltk_data]     ling\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Yee ling\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Yee\n",
      "[nltk_data]     ling\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Yee\n",
      "[nltk_data]     ling\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd  # pip install pandas\n",
    "import numpy as np  # pip install numpy\n",
    "import sklearn\n",
    "import imblearn\n",
    "\n",
    "import re, string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "# from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from imblearn.over_sampling  import RandomOverSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import make_pipeline\n",
    "\n",
    "\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Text Pre-processing\n",
    "#convert to lowercase, strip and remove punctuations\n",
    "def preprocess(text):\n",
    "    text = text.lower() \n",
    "    text= text.strip()  \n",
    "    text= re.compile('<.*?>').sub('', text) \n",
    "    text = re.compile('[%s]' % re.escape(string.punctuation)).sub(' ', text)  \n",
    "    text = re.sub('\\s+', ' ', text)  \n",
    "    text = re.sub(r'\\[[0-9]*\\]',' ',text) \n",
    "    text=re.sub(r'[^\\w\\s]', '', str(text).lower().strip())\n",
    "    text = re.sub(r'\\d',' ',text) \n",
    "    text = re.sub(r'\\s+',' ',text) \n",
    "    return text\n",
    "\n",
    " \n",
    "# STOPWORD REMOVAL\n",
    "def stopword(string):\n",
    "    a= [i for i in string.split() if i not in stopwords.words('english')]\n",
    "    return ' '.join(a)\n",
    "\n",
    "#LEMMATIZATION\n",
    "# Initialize the lemmatizer\n",
    "wl = WordNetLemmatizer()\n",
    " \n",
    "# This is a helper function to map NTLK position tags\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "# Tokenize the sentence\n",
    "def lemmatizer(string):\n",
    "    word_pos_tags = nltk.pos_tag(word_tokenize(string)) # Get position tags\n",
    "    a=[wl.lemmatize(tag[0], get_wordnet_pos(tag[1])) for idx, tag in enumerate(word_pos_tags)] # Map the position tag and lemmatize the word/token\n",
    "    return \" \".join(a)  \n",
    "\n",
    "def finalpreprocess(string):\n",
    "    return lemmatizer(stopword(preprocess(string)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load files\n",
    "\n",
    "df = pd.read_excel('train01.xlsx')\n",
    "dw = pd.read_excel('predict01.xlsx')\n",
    "\n",
    "df = df.dropna() #drop rows with missing values\n",
    "df = df.set_axis(['Text', 'Class'], axis=1, copy=False) #rename columns\n",
    "dw = dw.set_axis(['Text'], axis=1, copy=False) #rename columns\n",
    "\n",
    "df['Text'] = df['Text'].apply(lambda x: finalpreprocess(x))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['Text'],df['Class'], test_size=0.2) #split data into train and test sets\n",
    "\n",
    "# fit and apply the transform\n",
    "tv = TfidfVectorizer() \n",
    "ros = RandomOverSampler(sampling_strategy='minority')\n",
    "X_ROS, y_ROS = ros.fit_resample(tv.fit_transform(X_train), y_train)\n",
    "X_test = tv.transform(X_test)\n",
    "\n",
    "#Build a vectorizer / classifier pipeline that filters out tokens that are too rare or too frequent\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n",
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n",
      "                          0         1  \\\n",
      "0           MultinomialNB()       1.0   \n",
      "1                     SVC()       1.0   \n",
      "2  RandomForestClassifier()  0.712257   \n",
      "\n",
      "                                                   2  \n",
      "0                        (MultinomialNB(alpha=0.01))  \n",
      "1                                    (SVC(degree=1))  \n",
      "2  ((DecisionTreeClassifier(max_depth=20, max_fea...  \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.33      0.50         3\n",
      "           1       0.91      1.00      0.95        21\n",
      "\n",
      "    accuracy                           0.92        24\n",
      "   macro avg       0.96      0.67      0.73        24\n",
      "weighted avg       0.92      0.92      0.90        24\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Load Estimators\n",
    "rf=RandomForestClassifier()\n",
    "nb=MultinomialNB()\n",
    "svc=SVC()\n",
    "# rf=RandomForestClassifier(random_state=4, n_jobs=-1, max_features=\"sqrt\", warm_start=True)\n",
    "# nb=MultinomialNB(alpha=0.01)\n",
    "# svc=SVC(random_state=4, kernel='rbf')\n",
    "\n",
    "ensemble_clf=[rf, nb, svc] \n",
    "\n",
    "#Parameters\n",
    "paramrf={\"clf__max_depth\": range(10,30,10), \"clf__min_samples_leaf\": range(20,30,5),\n",
    "        \"clf__n_estimators\":range(500,800,200)}\n",
    "paramnb={\n",
    "\"clf__alpha\": [0.01, 0.1, 1.0]\n",
    "}\n",
    "paramsvc={\"clf__kernel\":[\"rbf\", \"poly\"], \"clf__gamma\": [\"auto\", \"scale\"], \"clf__degree\":range(1,6,1)}\n",
    "\n",
    "parameters_list=[paramrf, paramnb, paramsvc]\n",
    "model_log=pd.DataFrame([\"_rf\", \"_nb\", \"_svc\"])\n",
    "\n",
    "for i in range(len(ensemble_clf)): # ('tf-idf', TfidfVectorizer()),\n",
    "    Grid=GridSearchCV(Pipeline([ ('clf',ensemble_clf[i])]), param_grid=parameters_list[i], \n",
    "                    n_jobs=-1, cv=3, verbose=3, scoring='average_precision').fit(X_ROS, y_ROS)\n",
    "    model_log[i]=(ensemble_clf[i],Grid.best_score_, Grid.best_estimator_)  \n",
    "\n",
    "model_log= model_log.T\n",
    "model_log=model_log.sort_values(by=1, ascending=False).reset_index(drop = True)\n",
    "print(model_log)\n",
    "\n",
    "bestmodelt=model_log.loc[0][2]\n",
    "bestmodelt.fit(X_ROS, y_ROS)\n",
    "y_predicted = bestmodelt.predict(X_test)\n",
    "print(classification_report(y_test, y_predicted))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 Text  Predicted  \\\n",
      "0   This is a cute top with jeans for spring and s...          1   \n",
      "1   Petite pants hsould be able to fit short peple...          1   \n",
      "2   I tried on the petite size in my usual xs, adn...          1   \n",
      "3   I tried these on on a whim because i liked the...          1   \n",
      "4   The shirt is absolutely cute looking. but that...          1   \n",
      "5   These jeans! i tried these on, in addition to ...          1   \n",
      "6   I ordered this top in my usual size and am exc...          1   \n",
      "7   The blush stripes are subtle but they definite...          1   \n",
      "8   I got a small mauve. the fit is great and the ...          1   \n",
      "9   I have a similar pair of capris from retailer ...          1   \n",
      "10  Fun detail with the beading and lace! arms are...          1   \n",
      "11  I love the style of this top, and the longer l...          1   \n",
      "12  Ordered these online and they fit perfectly. i...          1   \n",
      "13  I loved this dress from the moment i tried it ...          1   \n",
      "14  I went ahead and ordered a size up based on pr...          1   \n",
      "15  I really want to love this shirt, but the smal...          0   \n",
      "16  Love pilcro, love the stripes and the length -...          1   \n",
      "17  This is exactly what i was expecting. cute, co...          1   \n",
      "18  I tried these on in the store, and they are su...          1   \n",
      "19  Really nice, substantial, fully lined sweater ...          1   \n",
      "\n",
      "                                           clean_Text  \n",
      "0   cute top jean spring summer warmer climate fre...  \n",
      "1   petite pant hsould able fit short peple inch i...  \n",
      "2   try petite size usual x adn actually go xxs lo...  \n",
      "3   try whim like shirt display store surprise muc...  \n",
      "4   shirt absolutely cute look look size wear l to...  \n",
      "5   jean try addition high rise paige denim hand c...  \n",
      "6   order top usual size exchange one size small x...  \n",
      "7   blush stripe subtle definitely give elongate e...  \n",
      "8   get small mauve fit great length perfect inch ...  \n",
      "9   similar pair capri retailer order think differ...  \n",
      "10  fun detail bead lace arm little long body swea...  \n",
      "11  love style top longer length would great leggi...  \n",
      "12  order online fit perfectly look lightweight pa...  \n",
      "13  love dress moment try flatter postpartum body ...  \n",
      "14  go ahead order size base previous review order...  \n",
      "15  really want love shirt small way big reference...  \n",
      "16  love pilcro love stripe length particular pair...  \n",
      "17  exactly expect cute comfortable casual gold se...  \n",
      "18  try store super cute run small typically wear ...  \n",
      "19  really nice substantial fully line sweater coa...  \n"
     ]
    }
   ],
   "source": [
    "#Predict the outcome on the new set and store it in a variable named w_predicted\n",
    "dw['clean_Text'] = dw['Text'].apply(lambda x: finalpreprocess(x))\n",
    "X_pred = tv.transform(dw['clean_Text'])\n",
    "w_predicted = bestmodelt.predict(X_pred)\n",
    "dw['Predicted'] = w_predicted\n",
    "dw=dw.drop(['clean_Text'], axis=1)\n",
    "print(dw)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c058de923858d6e5ced1e41dc5c877b1e9a3c6644c3607d8bdd7ab5ce0ec6d93"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
